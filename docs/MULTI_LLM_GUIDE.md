# ü§ñ Guide Multi-LLM - Sentry-AI

Ce guide explique comment utiliser diff√©rents fournisseurs de LLM (Large Language Models) avec Sentry-AI.

---

## üéØ Fournisseurs Support√©s

Sentry-AI supporte maintenant **4 fournisseurs de LLM** :

| Fournisseur | Type | Mod√®le par D√©faut | Avantages |
|-------------|------|-------------------|-----------|
| **Ollama** | Local | `phi3:mini` | üîí 100% priv√©, gratuit, offline |
| **Google Gemini** | Cloud | `gemini-2.0-flash-exp` | ‚ö° Rapide, performant, multimodal |
| **OpenAI** | Cloud | `gpt-4o-mini` | üéØ Pr√©cis, fiable, bien document√© |
| **Anthropic Claude** | Cloud | `claude-3-5-sonnet-20241022` | üß† Intelligent, contextuel, s√ªr |

---

## ‚öôÔ∏è Configuration

### 1. Choisir un Fournisseur

√âditez votre fichier `.env` :

```bash
# Choisissez un fournisseur
LLM_PROVIDER=ollama  # ou gemini, openai, claude

# Optionnel: sp√©cifiez un mod√®le personnalis√©
LLM_MODEL=  # Laissez vide pour utiliser le mod√®le par d√©faut

# Temp√©rature (0.0 = d√©terministe, 1.0 = cr√©atif)
LLM_TEMPERATURE=0.1
```

### 2. Configuration Sp√©cifique par Fournisseur

#### Option A: Ollama (Local)

**Avantages :** Gratuit, priv√©, offline  
**Inconv√©nients :** N√©cessite installation locale, moins performant

```bash
# Installation
brew install ollama
ollama serve  # Dans un terminal s√©par√©
ollama pull phi3:mini

# Configuration .env
LLM_PROVIDER=ollama
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=phi3:mini
```

#### Option B: Google Gemini

**Avantages :** Rapide, performant, gratuit (quota g√©n√©reux)  
**Inconv√©nients :** N√©cessite connexion internet, donn√©es envoy√©es √† Google

```bash
# 1. Obtenez une cl√© API
# Visitez: https://aistudio.google.com/apikey

# 2. Configuration .env
LLM_PROVIDER=gemini
GEMINI_API_KEY=votre_cle_api_ici
LLM_MODEL=gemini-2.0-flash-exp  # Optionnel
```

**Mod√®les disponibles :**
- `gemini-2.0-flash-exp` (recommand√© - rapide et performant)
- `gemini-1.5-pro` (plus puissant mais plus lent)
- `gemini-1.5-flash` (√©quilibr√©)

#### Option C: OpenAI

**Avantages :** Tr√®s performant, bien document√©  
**Inconv√©nients :** Payant, n√©cessite connexion internet

```bash
# 1. Obtenez une cl√© API
# Visitez: https://platform.openai.com/api-keys

# 2. Configuration .env
LLM_PROVIDER=openai
OPENAI_API_KEY=votre_cle_api_ici
LLM_MODEL=gpt-4o-mini  # Optionnel
```

**Mod√®les disponibles :**
- `gpt-4o-mini` (recommand√© - rapide et √©conomique)
- `gpt-4o` (plus puissant)
- `gpt-4-turbo` (√©quilibr√©)

#### Option D: Anthropic Claude

**Avantages :** Tr√®s intelligent, excellent pour le contexte  
**Inconv√©nients :** Payant, n√©cessite connexion internet

```bash
# 1. Obtenez une cl√© API
# Visitez: https://console.anthropic.com/settings/keys

# 2. Configuration .env
LLM_PROVIDER=claude
ANTHROPIC_API_KEY=votre_cle_api_ici
LLM_MODEL=claude-3-5-sonnet-20241022  # Optionnel
```

**Mod√®les disponibles :**
- `claude-3-5-sonnet-20241022` (recommand√© - √©quilibr√©)
- `claude-3-opus-20240229` (plus puissant)
- `claude-3-haiku-20240307` (plus rapide)

---

## üß™ Tester Votre Configuration

Utilisez le script de test int√©gr√© :

```bash
python -c "
from sentry_ai.core.llm_provider import LLMProviderFactory
from sentry_ai.core.config import settings

# Afficher le fournisseur configur√©
print(f'Provider: {settings.llm_provider}')

# Tester la disponibilit√©
available = LLMProviderFactory.get_available_providers()
print(f'Available providers: {[p.value for p in available]}')
"
```

---

## üí° Recommandations

### Pour le D√©veloppement

**Utilisez Ollama** :
- ‚úÖ Gratuit
- ‚úÖ Pas besoin de cl√© API
- ‚úÖ Fonctionne offline
- ‚úÖ Donn√©es priv√©es

### Pour la Production

**Utilisez Gemini ou OpenAI** :
- ‚úÖ Plus performant
- ‚úÖ Plus rapide
- ‚úÖ Meilleure compr√©hension du contexte
- ‚ö†Ô∏è N√©cessite cl√© API
- ‚ö†Ô∏è Co√ªts √† pr√©voir

### Pour la Confidentialit√© Maximale

**Utilisez Ollama uniquement** :
- ‚úÖ 100% local
- ‚úÖ Aucune donn√©e envoy√©e √† l'ext√©rieur
- ‚úÖ Conforme RGPD

---

## üîÑ Changer de Fournisseur

Vous pouvez changer de fournisseur √† tout moment :

1. √âditez `.env`
2. Changez `LLM_PROVIDER`
3. Ajoutez la cl√© API si n√©cessaire
4. Red√©marrez Sentry-AI

```bash
# Exemple: passer d'Ollama √† Gemini
LLM_PROVIDER=gemini
GEMINI_API_KEY=votre_cle_ici
```

---

## üìä Comparaison des Performances

| Crit√®re | Ollama | Gemini | OpenAI | Claude |
|---------|--------|--------|--------|--------|
| **Vitesse** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Pr√©cision** | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Co√ªt** | Gratuit | Gratuit* | Payant | Payant |
| **Confidentialit√©** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **Offline** | ‚úÖ | ‚ùå | ‚ùå | ‚ùå |

*Gemini offre un quota gratuit g√©n√©reux

---

## üÜò D√©pannage

### "Provider not available"

**Solution :** V√©rifiez que :
1. La cl√© API est correcte (pour cloud providers)
2. Ollama est d√©marr√© (pour Ollama)
3. Les d√©pendances sont install√©es (`pip install -r requirements.txt`)

### "API key not found"

**Solution :** Ajoutez la cl√© API dans `.env` :
```bash
GEMINI_API_KEY=votre_cle
OPENAI_API_KEY=votre_cle
ANTHROPIC_API_KEY=votre_cle
```

### "Model not found"

**Solution :** V√©rifiez le nom du mod√®le dans la documentation du fournisseur.

---

## üìù Notes Importantes

1. **S√©curit√© :** Ne commitez JAMAIS vos cl√©s API dans Git
2. **Co√ªts :** Surveillez votre utilisation pour les providers payants
3. **Confidentialit√© :** Utilisez Ollama si vous traitez des donn√©es sensibles
4. **Performance :** Les providers cloud sont g√©n√©ralement plus rapides

---

**D√©velopp√© avec ‚ù§Ô∏è pour la communaut√© macOS**
