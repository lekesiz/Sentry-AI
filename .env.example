# Sentry-AI Configuration Example
# Copy this file to .env and customize as needed

# Application Settings
DEBUG=False
LOG_LEVEL=INFO
LOG_FILE=sentry_ai.log
LOG_RETENTION_DAYS=30

# LLM Provider Configuration
# Choose one: ollama, gemini, openai, claude
LLM_PROVIDER=ollama
LLM_MODEL=  # Leave empty for provider default
LLM_TEMPERATURE=0.1

# Ollama Configuration (when using ollama provider)
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=phi3:mini

# API Keys (required for cloud providers)
# Get your keys from:
# - Gemini: https://aistudio.google.com/apikey
# - OpenAI: https://platform.openai.com/api-keys
# - Claude: https://console.anthropic.com/settings/keys
GEMINI_API_KEY=
OPENAI_API_KEY=
ANTHROPIC_API_KEY=

# LLM Fallback Configuration
# Enable automatic fallback to other providers if one fails
LLM_FALLBACK_ENABLED=True
# Order of providers to try (JSON array format)
LLM_FALLBACK_ORDER=["claude","openai","gemini","ollama"]

# Observer Settings
OBSERVER_INTERVAL=2.0
OBSERVER_ENABLED=True

# Database Settings
DATABASE_URL=sqlite:///./sentry_ai.db

# API Settings
API_HOST=127.0.0.1
API_PORT=8000

# Security Settings (JSON array format)
BLACKLIST_APPS=[]
# WHITELIST_APPS=[]  # If set, only these apps will be automated
REQUIRE_CONFIRMATION_FOR=[]
